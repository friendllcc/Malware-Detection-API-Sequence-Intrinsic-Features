import os
import numpy as np
import pandas as pd
from keras.preprocessing import sequence
from keras.layers.embeddings import Embedding
from keras.layers import Dense, LSTM, Bidirectional, Dropout, Conv1D, concatenate
from keras.layers import Dense, Flatten, Dropout, GlobalMaxPooling1D, BatchNormalization
from keras.models import Sequential, load_model, Model
from keras import regularizers, Input
from keras.optimizers import Adam
from keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score


# proposed model
def create_model(data_length, embedding_output_dim, embedding_output_dim_semantic, CNN_filters, LSTM_units,
                 dropout_rate, DNN_size1, DNN_size2):
    # Model Input
    train_x_name = Input(shape=(data_length,), dtype='float64', name='train_x_name')
    train_x_semantic = Input(shape=(data_length * 4,), dtype='float64', name='train_x_semantic')  # Semantic Chain

    # Word Embedding
    embedder1 = Embedding(input_dim=316, output_dim=embedding_output_dim, input_length=data_length)
    embedder2 = Embedding(input_dim=289, output_dim=embedding_output_dim_semantic, input_length=data_length * 4)

    embed1 = embedder1(train_x_name)
    embed2 = embedder2(train_x_semantic)

    # API Phrase Feature Maps: three convolution layers with filter size h = 3, 4, 5
    cnn1_1 = Conv1D(filters=CNN_filters, kernel_size=3, padding='same', strides=1, activation='relu')(embed1)
    cnn1_2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=1, activation='relu')(embed1)
    cnn1_3 = Conv1D(filters=CNN_filters, kernel_size=5, padding='same', strides=1, activation='relu')(embed1)

    # Semantic Chain Feature Maps
    cnn2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=4, activation='relu')(embed2)

    # Feature Concatenate
    con = concatenate([cnn1_1, cnn1_2, cnn1_3, cnn2], axis=-1)

    # Bi-LSTM
    lstm = Bidirectional(LSTM(units=LSTM_units, return_sequences=True, activation='sigmoid'))(con)
    flat = Flatten()(lstm)

    # Classification
    drop = Dropout(dropout_rate)(flat)
    dnn1 = Dense(DNN_size1, activation='relu')(drop)
    drop2 = Dropout(dropout_rate)(dnn1)
    dnn2 = Dense(DNN_size2, activation='relu')(drop2)
    main_output = Dense(1, activation='sigmoid')(dnn2)

    model = Model(inputs=[train_x_name, train_x_semantic], outputs=main_output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=None)  # optimizer='adam'
    model.summary()
    return model


# model without API Phrase
def create_model_without_API_Phrase(data_length, embedding_output_dim, embedding_output_dim_semantic, CNN_filters,
                                    LSTM_units, dropout_rate, DNN_size1, DNN_size2):
    # Model Input
    train_x_name = Input(shape=(data_length,), dtype='float64', name='train_x_name')
    train_x_semantic = Input(shape=(data_length * 4,), dtype='float64', name='train_x_semantic')  # Semantic Chain

    # Word Embedding
    embedder2 = Embedding(input_dim=289, output_dim=embedding_output_dim_semantic, input_length=data_length * 4)

    embed2 = embedder2(train_x_semantic)

    # Semantic Chain Feature Maps
    cnn2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=4, activation='relu')(embed2)

    # Bi-LSTM
    lstm = Bidirectional(LSTM(units=LSTM_units, return_sequences=True, activation='sigmoid'))(cnn2)
    flat = Flatten()(lstm)

    # Classification
    drop = Dropout(dropout_rate)(flat)
    dnn1 = Dense(DNN_size1, activation='relu')(drop)
    drop2 = Dropout(dropout_rate)(dnn1)
    dnn2 = Dense(DNN_size2, activation='relu')(drop2)
    main_output = Dense(1, activation='sigmoid')(dnn2)

    model = Model(inputs=[train_x_name, train_x_semantic], outputs=main_output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=None)  # optimizer='adam'
    model.summary()
    return model


# model without Semantic Chain
def create_model_without_Semantic_Chain(data_length, embedding_output_dim, embedding_output_dim_semantic, CNN_filters,
                                        LSTM_units, dropout_rate, DNN_size1, DNN_size2):
    # Model Input
    train_x_name = Input(shape=(data_length,), dtype='float64', name='train_x_name')
    train_x_semantic = Input(shape=(data_length * 4,), dtype='float64', name='train_x_semantic')  # Semantic Chain

    # Word Embedding
    embedder1 = Embedding(input_dim=316, output_dim=embedding_output_dim, input_length=data_length)

    embed1 = embedder1(train_x_name)

    # API Phrase Feature Maps: three convolution layers with filter size h = 3, 4, 5
    cnn1_1 = Conv1D(filters=CNN_filters, kernel_size=3, padding='same', strides=1, activation='relu')(embed1)
    cnn1_2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=1, activation='relu')(embed1)
    cnn1_3 = Conv1D(filters=CNN_filters, kernel_size=5, padding='same', strides=1, activation='relu')(embed1)

    # Feature Concatenate
    con = concatenate([cnn1_1, cnn1_2, cnn1_3], axis=-1)

    # Bi-LSTM
    lstm = Bidirectional(LSTM(units=LSTM_units, return_sequences=True, activation='sigmoid'))(con)
    flat = Flatten()(lstm)

    # Classification
    drop = Dropout(dropout_rate)(flat)
    dnn1 = Dense(DNN_size1, activation='relu')(drop)
    drop2 = Dropout(dropout_rate)(dnn1)
    dnn2 = Dense(DNN_size2, activation='relu')(drop2)
    main_output = Dense(1, activation='sigmoid')(dnn2)

    model = Model(inputs=[train_x_name, train_x_semantic], outputs=main_output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=None)  # optimizer='adam'
    model.summary()
    return model


# model without Bi-LSTM
def create_model_without_BiLSTM(data_length, embedding_output_dim, embedding_output_dim_semantic, CNN_filters,
                                LSTM_units, dropout_rate, DNN_size1, DNN_size2):
    # Model Input
    train_x_name = Input(shape=(data_length,), dtype='float64', name='train_x_name')
    train_x_semantic = Input(shape=(data_length * 4,), dtype='float64', name='train_x_semantic')  # Semantic Chain

    # Word Embedding
    embedder1 = Embedding(input_dim=316, output_dim=embedding_output_dim, input_length=data_length)
    embedder2 = Embedding(input_dim=289, output_dim=embedding_output_dim_semantic, input_length=data_length * 4)

    embed1 = embedder1(train_x_name)
    embed2 = embedder2(train_x_semantic)

    # API Phrase Feature Maps: three convolution layers with filter size h = 3, 4, 5
    cnn1_1 = Conv1D(filters=CNN_filters, kernel_size=3, padding='same', strides=1, activation='relu')(embed1)
    cnn1_2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=1, activation='relu')(embed1)
    cnn1_3 = Conv1D(filters=CNN_filters, kernel_size=5, padding='same', strides=1, activation='relu')(embed1)

    # Semantic Chain Feature Maps
    cnn2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=4, activation='relu')(embed2)

    # Feature Concatenate
    con = concatenate([cnn1_1, cnn1_2, cnn1_3, cnn2], axis=-1)

    flat = Flatten()(con)

    # Classification
    drop = Dropout(dropout_rate)(flat)
    dnn1 = Dense(DNN_size1, activation='relu')(drop)
    drop2 = Dropout(dropout_rate)(dnn1)
    dnn2 = Dense(DNN_size2, activation='relu')(drop2)
    main_output = Dense(1, activation='sigmoid')(dnn2)

    model = Model(inputs=[train_x_name, train_x_semantic], outputs=main_output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=None)  # optimizer='adam'
    model.summary()
    return model


# model without Classification Module
def create_model_without_Classification_Module(data_length, embedding_output_dim, embedding_output_dim_semantic,
                                               CNN_filters, LSTM_units, dropout_rate, DNN_size1, DNN_size2):
    # Model Input
    train_x_name = Input(shape=(data_length,), dtype='float64', name='train_x_name')
    train_x_semantic = Input(shape=(data_length * 4,), dtype='float64', name='train_x_semantic')  # Semantic Chain

    # Word Embedding
    embedder1 = Embedding(input_dim=316, output_dim=embedding_output_dim, input_length=data_length)
    embedder2 = Embedding(input_dim=289, output_dim=embedding_output_dim_semantic, input_length=data_length * 4)

    embed1 = embedder1(train_x_name)
    embed2 = embedder2(train_x_semantic)

    # API Phrase Feature Maps: three convolution layers with filter size h = 3, 4, 5
    cnn1_1 = Conv1D(filters=CNN_filters, kernel_size=3, padding='same', strides=1, activation='relu')(embed1)
    cnn1_2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=1, activation='relu')(embed1)
    cnn1_3 = Conv1D(filters=CNN_filters, kernel_size=5, padding='same', strides=1, activation='relu')(embed1)

    # Semantic Chain Feature Maps
    cnn2 = Conv1D(filters=CNN_filters, kernel_size=4, padding='same', strides=4, activation='relu')(embed2)

    # Feature Concatenate
    con = concatenate([cnn1_1, cnn1_2, cnn1_3, cnn2], axis=-1)

    # Bi-LSTM
    lstm = Bidirectional(LSTM(units=LSTM_units, return_sequences=True, activation='sigmoid'))(con)
    flat = Flatten()(lstm)

    # Classification
    main_output = Dense(1, activation='sigmoid')(flat)

    model = Model(inputs=[train_x_name, train_x_semantic], outputs=main_output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=None)  # optimizer='adam'
    model.summary()
    return model
